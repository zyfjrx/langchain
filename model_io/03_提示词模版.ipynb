{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### model io 提示词模版",
   "id": "a1d3035c407c601e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 1.PromptTemplate",
   "id": "61990274647cb17b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:08:28.207783Z",
     "start_time": "2025-08-05T06:08:28.203097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "    你是一个人工智能助手。\n",
    "    你的名字是：{input}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"input\"])\n",
    "str = prompt.format(input=\"小智\")\n",
    "print(str)\n",
    "print(type(str))"
   ],
   "id": "f204cfab38ad2563",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    你是一个人工智能助手。\n",
      "    你的名字是：小智\n",
      "    \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:15:12.809852Z",
     "start_time": "2025-08-05T06:15:12.806284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"\"\"\n",
    "    你是一个人工智能助手。\n",
    "    你的名字是：{input}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "prompt.format(input=\"小智\")\n",
    "print(str)\n",
    "print(type(str))"
   ],
   "id": "40c2b5517301f981",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    你是一个人工智能助手。\n",
      "    你的名字是：小智\n",
      "    \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "部分提示词模版",
   "id": "963590da240da438"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:35:30.549900Z",
     "start_time": "2025-08-05T06:35:30.546397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    template=\"请评价{product}的优缺点，包括{aspect1}和{aspect2}。\",\n",
    "    partial_variables={\"aspect1\": \"电池续航\", \"aspect2\": \"拍照质量\"})\n",
    "\n",
    "#使用模板生成提示词\n",
    "prompt_1 = template.format(product=\"智能手机\")\n",
    "print(prompt_1)"
   ],
   "id": "d586e68b40236548",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请评价智能手机的优缺点，包括电池续航和拍照质量。\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:42:57.067026Z",
     "start_time": "2025-08-05T06:42:57.063303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "template = PromptTemplate.from_template(\n",
    "    template=\"请评价{product}的优缺点，包括{aspect1}和{aspect2}。\",\n",
    "    )\n",
    "\n",
    "#使用模板生成提示词\n",
    "template1 = template.partial(aspect1=\"电池续航\", aspect2=\"拍照质量\")\n",
    "prompt_1 = template1.format(product=\"智能手机\")\n",
    "print(prompt_1)"
   ],
   "id": "1e5844fd6d080788",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请评价智能手机的优缺点，包括电池续航和拍照质量。\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "组合提示词模版",
   "id": "904117dd0da8b083"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:48:59.787195Z",
     "start_time": "2025-08-05T06:48:59.783775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "    + \", make it funny\"\n",
    "    + \"\\n\\nand in {language}\"\n",
    ")\n",
    "\n",
    "prompt = template.format(topic=\"sports\", language=\"spanish\")\n",
    "print(prompt)"
   ],
   "id": "3ce0c4768dcd236e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me a joke about sports, make it funny\n",
      "\n",
      "and in spanish\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "invoke()替换format()",
   "id": "8462e2719fc2c4f9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:52:23.311731Z",
     "start_time": "2025-08-05T06:52:23.307635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "template = \"\"\"\n",
    "    你是一个人工智能助手。\n",
    "    你的名字是：{input}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "str = prompt.format(input=\"小智\")\n",
    "print(str)\n",
    "print(type(str))"
   ],
   "id": "373e241022bfbe99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    你是一个人工智能助手。\n",
      "    你的名字是：小智\n",
      "    \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T06:52:59.279926Z",
     "start_time": "2025-08-05T06:52:59.274220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "template = \"\"\"\n",
    "    你是一个人工智能助手。\n",
    "    你的名字是：{input}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate.from_template(template=template)\n",
    "str = prompt.invoke(input={\"input\": \"小智\"})\n",
    "print(str)\n",
    "print(type(str))"
   ],
   "id": "11d21391efd3997",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='\\n    你是一个人工智能助手。\\n    你的名字是：小智\\n    '\n",
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:17:15.571038Z",
     "start_time": "2025-08-05T08:17:04.670260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"),base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"您是一位专业的程序员。\\n对于信息 {text} 进行简短描述\"\n",
    ")\n",
    "# 输入提示\n",
    "prompt = prompt_template.invoke(input={\"text\": \"langchain\"})\n",
    "\n",
    "# 得到模型的输出\n",
    "output = llm.invoke(prompt)\n",
    "\n",
    "# 打印输出内容\n",
    "print(output)\n",
    "print(type(output))"
   ],
   "id": "6743e1bbd997bd7c",
   "outputs": [
    {
     "ename": "APIConnectionError",
     "evalue": "Connection error.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_transports/default.py:101\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[0;34m()\u001B[0m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 101\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_transports/default.py:250\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[0;32m--> 250\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(resp\u001B[38;5;241m.\u001B[39mstream, typing\u001B[38;5;241m.\u001B[39mIterable)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:256\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_close_connections(closing)\n\u001B[0;32m--> 256\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exc \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    258\u001B[0m \u001B[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001B[39;00m\n\u001B[1;32m    259\u001B[0m \u001B[38;5;66;03m# the point at which the response is closed.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpcore/_sync/connection_pool.py:236\u001B[0m, in \u001B[0;36mConnectionPool.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;66;03m# Send the request on the assigned connection.\u001B[39;00m\n\u001B[0;32m--> 236\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    237\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpool_request\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ConnectionNotAvailable:\n\u001B[1;32m    240\u001B[0m     \u001B[38;5;66;03m# In some cases a connection may initially be available to\u001B[39;00m\n\u001B[1;32m    241\u001B[0m     \u001B[38;5;66;03m# handle a request, but then become unavailable.\u001B[39;00m\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;66;03m# In this case we clear the connection and try again.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpcore/_sync/http_proxy.py:316\u001B[0m, in \u001B[0;36mTunnelHTTPConnection.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Trace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstart_tls\u001B[39m\u001B[38;5;124m\"\u001B[39m, logger, request, kwargs) \u001B[38;5;28;01mas\u001B[39;00m trace:\n\u001B[0;32m--> 316\u001B[0m     stream \u001B[38;5;241m=\u001B[39m \u001B[43mstream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_tls\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    317\u001B[0m     trace\u001B[38;5;241m.\u001B[39mreturn_value \u001B[38;5;241m=\u001B[39m stream\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpcore/_sync/http11.py:376\u001B[0m, in \u001B[0;36mHTTP11UpgradeStream.start_tls\u001B[0;34m(self, ssl_context, server_hostname, timeout)\u001B[0m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mstart_tls\u001B[39m(\n\u001B[1;32m    371\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    372\u001B[0m     ssl_context: ssl\u001B[38;5;241m.\u001B[39mSSLContext,\n\u001B[1;32m    373\u001B[0m     server_hostname: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    374\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    375\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NetworkStream:\n\u001B[0;32m--> 376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_stream\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart_tls\u001B[49m\u001B[43m(\u001B[49m\u001B[43mssl_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mserver_hostname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpcore/_backends/sync.py:154\u001B[0m, in \u001B[0;36mSyncStream.start_tls\u001B[0;34m(self, ssl_context, server_hostname, timeout)\u001B[0m\n\u001B[1;32m    150\u001B[0m exc_map: ExceptionMapping \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    151\u001B[0m     socket\u001B[38;5;241m.\u001B[39mtimeout: ConnectTimeout,\n\u001B[1;32m    152\u001B[0m     \u001B[38;5;167;01mOSError\u001B[39;00m: ConnectError,\n\u001B[1;32m    153\u001B[0m }\n\u001B[0;32m--> 154\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_exceptions(exc_map):\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/contextlib.py:153\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[0;34m(self, typ, value, traceback)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 153\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpcore/_exceptions.py:14\u001B[0m, in \u001B[0;36mmap_exceptions\u001B[0;34m(map)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(exc, from_exc):\n\u001B[0;32m---> 14\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m to_exc(exc) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[0;31mConnectError\u001B[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mConnectError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/openai/_base_client.py:979\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 979\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    981\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_should_stream_response_body\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    982\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    983\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    984\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m httpx\u001B[38;5;241m.\u001B[39mTimeoutException \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_client.py:914\u001B[0m, in \u001B[0;36mClient.send\u001B[0;34m(self, request, stream, auth, follow_redirects)\u001B[0m\n\u001B[1;32m    912\u001B[0m auth \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_request_auth(request, auth)\n\u001B[0;32m--> 914\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_auth\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    915\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    916\u001B[0m \u001B[43m    \u001B[49m\u001B[43mauth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mauth\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    917\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    918\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    920\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_client.py:942\u001B[0m, in \u001B[0;36mClient._send_handling_auth\u001B[0;34m(self, request, auth, follow_redirects, history)\u001B[0m\n\u001B[1;32m    941\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m--> 942\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_handling_redirects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    943\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    944\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfollow_redirects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfollow_redirects\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhistory\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhistory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    947\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_client.py:979\u001B[0m, in \u001B[0;36mClient._send_handling_redirects\u001B[0;34m(self, request, follow_redirects, history)\u001B[0m\n\u001B[1;32m    977\u001B[0m     hook(request)\n\u001B[0;32m--> 979\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_single_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_client.py:1014\u001B[0m, in \u001B[0;36mClient._send_single_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m   1013\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m request_context(request\u001B[38;5;241m=\u001B[39mrequest):\n\u001B[0;32m-> 1014\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[43mtransport\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhandle_request\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1016\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response\u001B[38;5;241m.\u001B[39mstream, SyncByteStream)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_transports/default.py:249\u001B[0m, in \u001B[0;36mHTTPTransport.handle_request\u001B[0;34m(self, request)\u001B[0m\n\u001B[1;32m    237\u001B[0m req \u001B[38;5;241m=\u001B[39m httpcore\u001B[38;5;241m.\u001B[39mRequest(\n\u001B[1;32m    238\u001B[0m     method\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[1;32m    239\u001B[0m     url\u001B[38;5;241m=\u001B[39mhttpcore\u001B[38;5;241m.\u001B[39mURL(\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    247\u001B[0m     extensions\u001B[38;5;241m=\u001B[39mrequest\u001B[38;5;241m.\u001B[39mextensions,\n\u001B[1;32m    248\u001B[0m )\n\u001B[0;32m--> 249\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m map_httpcore_exceptions():\n\u001B[1;32m    250\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pool\u001B[38;5;241m.\u001B[39mhandle_request(req)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/contextlib.py:153\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__exit__\u001B[0;34m(self, typ, value, traceback)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 153\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthrow\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtyp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001B[39;00m\n\u001B[1;32m    156\u001B[0m     \u001B[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001B[39;00m\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/httpx/_transports/default.py:118\u001B[0m, in \u001B[0;36mmap_httpcore_exceptions\u001B[0;34m()\u001B[0m\n\u001B[1;32m    117\u001B[0m message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mstr\u001B[39m(exc)\n\u001B[0;32m--> 118\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m mapped_exc(message) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mexc\u001B[39;00m\n",
      "\u001B[0;31mConnectError\u001B[0m: [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1017)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mAPIConnectionError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[47], line 17\u001B[0m\n\u001B[1;32m     14\u001B[0m prompt \u001B[38;5;241m=\u001B[39m prompt_template\u001B[38;5;241m.\u001B[39minvoke(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlangchain\u001B[39m\u001B[38;5;124m\"\u001B[39m})\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# 得到模型的输出\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# 打印输出内容\u001B[39;00m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(output)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:389\u001B[0m, in \u001B[0;36mBaseLLM.invoke\u001B[0;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    385\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    386\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    387\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 389\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcallbacks\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtags\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_name\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrun_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    399\u001B[0m         \u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    400\u001B[0m         \u001B[38;5;241m.\u001B[39mtext\n\u001B[1;32m    401\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:766\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    757\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[1;32m    758\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    759\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    763\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    764\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    765\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 766\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:971\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    956\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    957\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    958\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    959\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    969\u001B[0m         )\n\u001B[1;32m    970\u001B[0m     ]\n\u001B[0;32m--> 971\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    972\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    973\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    974\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    975\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    976\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    977\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    979\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    980\u001B[0m         callback_managers[idx]\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    981\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_serialized,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    988\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m missing_prompt_idxs\n\u001B[1;32m    989\u001B[0m     ]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_core/language_models/llms.py:792\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    782\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    783\u001B[0m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    788\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    789\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    790\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    791\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 792\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    796\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    799\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    800\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    801\u001B[0m         )\n\u001B[1;32m    802\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    803\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/langchain_openai/llms/base.py:332\u001B[0m, in \u001B[0;36mBaseOpenAI._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    316\u001B[0m     choices\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m    317\u001B[0m         {\n\u001B[1;32m    318\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m\"\u001B[39m: generation\u001B[38;5;241m.\u001B[39mtext,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    329\u001B[0m         }\n\u001B[1;32m    330\u001B[0m     )\n\u001B[1;32m    331\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 332\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_prompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    333\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[1;32m    334\u001B[0m         \u001B[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001B[39;00m\n\u001B[1;32m    335\u001B[0m         \u001B[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001B[39;00m\n\u001B[1;32m    336\u001B[0m         response \u001B[38;5;241m=\u001B[39m response\u001B[38;5;241m.\u001B[39mmodel_dump()\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/openai/_utils/_utils.py:287\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    285\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/openai/resources/completions.py:541\u001B[0m, in \u001B[0;36mCompletions.create\u001B[0;34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m    513\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[1;32m    514\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    539\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[1;32m    540\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Completion \u001B[38;5;241m|\u001B[39m Stream[Completion]:\n\u001B[0;32m--> 541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    542\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    543\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    544\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest_of\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    548\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mecho\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mecho\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    549\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msuffix\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    564\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[1;32m    570\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    571\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    573\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    574\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/openai/_base_client.py:1256\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1242\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1243\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1244\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1251\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1252\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m   1253\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1254\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1255\u001B[0m     )\n\u001B[0;32m-> 1256\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/openai/_base_client.py:1011\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1008\u001B[0m         \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[1;32m   1010\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRaising connection error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1011\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m APIConnectionError(request\u001B[38;5;241m=\u001B[39mrequest) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m   1013\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m   1014\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mHTTP Response: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m   1015\u001B[0m     request\u001B[38;5;241m.\u001B[39mmethod,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1019\u001B[0m     response\u001B[38;5;241m.\u001B[39mheaders,\n\u001B[1;32m   1020\u001B[0m )\n\u001B[1;32m   1021\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequest_id: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, response\u001B[38;5;241m.\u001B[39mheaders\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx-request-id\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "\u001B[0;31mAPIConnectionError\u001B[0m: Connection error."
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T07:04:54.629245Z",
     "start_time": "2025-08-05T07:04:52.717148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "llm = ChatOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"),base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"text\"],\n",
    "    template=\"您是一位专业的程序员。\\n对于信息 {text} 进行简短描述\"\n",
    ")\n",
    "# 输入提示\n",
    "prompt = prompt_template.invoke(input={\"text\": \"langchain\"})\n",
    "\n",
    "# 得到模型的输出\n",
    "message = llm.invoke(prompt)\n",
    "\n",
    "# 打印输出内容\n",
    "print(message)\n",
    "print(type(message))"
   ],
   "id": "97efc2c5e4ecf765",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain 是一个用于构建语言模型应用的框架，它提供了一系列工具和组件，让开发者能够更轻松地构建和管理与语言模型交互的应用。LangChain 支持文本生成、问答、对话系统等多种功能，能够整合多种数据源和 API，帮助开发者提升开发效率和应用灵活性。通过 LangChain，开发者可以快速构建出复杂的语言处理流程，适应不同的业务需求。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 24, 'total_tokens': 129, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-C1617s1HwiQLgimhPKfpeHmwRDXRP', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--8834e963-4ffb-497c-ab37-a45773c767df-0' usage_metadata={'input_tokens': 24, 'output_tokens': 105, 'total_tokens': 129, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2.ChatPromptTemplate",
   "id": "5cfaa97bbb43432e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "实例化方式：\n",
    "- `from_message`\n",
    "- `构造方法`"
   ],
   "id": "b043dfc7ae2f0a09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T07:18:36.068055Z",
     "start_time": "2025-08-05T07:18:36.065390Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造方法\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate(messages=[\n",
    "    (\"system\", \"你是一个专业的程序员。你的名字是{name}\"),\n",
    "    (\"human\", \"对于信息 {text} 进行简短描述\"),\n",
    "])\n",
    "prompt = prompt_template.format(name=\"小智\", text=\"langchain\")\n",
    "print(prompt)\n",
    "print(type(prompt))"
   ],
   "id": "6e35ebbcf849ac04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 你是一个专业的程序员。你的名字是小智\n",
      "Human: 对于信息 langchain 进行简短描述\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T07:20:08.986730Z",
     "start_time": "2025-08-05T07:20:08.983370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from_message\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages=[\n",
    "    (\"system\", \"你是一个专业的程序员。你的名字是{name}\"),\n",
    "    (\"human\", \"对于信息 {text} 进行简短描述\"),\n",
    "])\n",
    "prompt = prompt_template.format(name=\"小智\", text=\"langchain\")\n",
    "print(prompt)\n",
    "print(type(prompt))"
   ],
   "id": "7ba9e4acf6e14fb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 你是一个专业的程序员。你的名字是小智\n",
      "Human: 对于信息 langchain 进行简短描述\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "调用方式：\n",
    "- `invoke`\n",
    "- `format`\n",
    "- `format_prompt`\n",
    "- `format_messages`"
   ],
   "id": "2e3a184b9d48d404"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T07:25:40.375889Z",
     "start_time": "2025-08-05T07:25:40.370221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages=[\n",
    "    (\"system\", \"你是一个专业的程序员。你的名字是{name}\"),\n",
    "    (\"human\", \"对于信息 {text} 进行简短描述\"),\n",
    "])\n",
    "prompt = prompt_template.invoke(input={\"name\": \"小智\", \"text\": \"langchain\"})\n",
    "print(prompt)\n",
    "print(type(prompt))"
   ],
   "id": "2ecb74c30210219d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='你是一个专业的程序员。你的名字是小智', additional_kwargs={}, response_metadata={}), HumanMessage(content='对于信息 langchain 进行简短描述', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.prompt_values.ChatPromptValue'>\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T07:34:50.032614Z",
     "start_time": "2025-08-05T07:34:50.029421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages=[\n",
    "    (\"system\", \"你是一个专业的程序员。你的名字是{name}\"),\n",
    "    (\"human\", \"对于信息 {text} 进行简短描述\"),\n",
    "])\n",
    "prompt = prompt_template.format_prompt(name=\"小智\", text=\"langchain\")\n",
    "# print(prompt)\n",
    "# print(type(prompt))\n",
    "print(prompt.to_messages())\n",
    "print(type(prompt.to_messages()))"
   ],
   "id": "98dd450840f0a6ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个专业的程序员。你的名字是小智', additional_kwargs={}, response_metadata={}), HumanMessage(content='对于信息 langchain 进行简短描述', additional_kwargs={}, response_metadata={})]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T07:29:53.819783Z",
     "start_time": "2025-08-05T07:29:53.816293Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages=[\n",
    "    (\"system\", \"你是一个专业的程序员。你的名字是{name}\"),\n",
    "    (\"human\", \"对于信息 {text} 进行简短描述\"),\n",
    "])\n",
    "prompt = prompt_template.format_messages(name=\"小智\", text=\"langchain\")\n",
    "print(prompt)\n",
    "print(type(prompt))"
   ],
   "id": "c06812d6999900b1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个专业的程序员。你的名字是小智', additional_kwargs={}, response_metadata={}), HumanMessage(content='对于信息 langchain 进行简短描述', additional_kwargs={}, response_metadata={})]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "几种不同类型的参数情况",
   "id": "1f9edd19360426"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:00:00.588392Z",
     "start_time": "2025-08-05T08:00:00.584520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages=[\n",
    "    \"Hello, {name}!\",\n",
    "])\n",
    "prompt = prompt_template.format_messages(name=\"小智\")\n",
    "print(prompt)\n",
    "print(type(prompt))"
   ],
   "id": "375cf3cbbc70ff68",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Hello, 小智!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, 小智!', additional_kwargs={}, response_metadata={})]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:08:21.741086Z",
     "start_time": "2025-08-05T08:08:21.738606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入先关包\n",
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "\n",
    "# 2.定义模版\n",
    "prompt = \"愿{subject}与你同在 \"\n",
    "\n",
    "# 3.创建自定义角色聊天消息提示词模版\n",
    "chat_message_prompt = ChatMessagePromptTemplate.from_template(\n",
    "    role=\"绝地武士\", template=prompt\n",
    ")\n",
    "# 4.格式聊天消息提示词\n",
    "resp = chat_message_prompt.format(subject=\"力\")\n",
    "print(type(resp))\n",
    "print(resp)"
   ],
   "id": "a74d8cdae0f94015",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.chat.ChatMessage'>\n",
      "content='愿力与你同在 ' additional_kwargs={} response_metadata={} role='绝地武士'\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:09:18.940229Z",
     "start_time": "2025-08-05T08:09:18.937119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入聊天消息类模板\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "\n",
    "# 创建消息模板\n",
    "system_template = \"你是一个专家{role}\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)\n",
    "\n",
    "human_template = \"给我解释{concept}，用浅显易懂的语言\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# 组合成聊天提示模板\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# 格式化提示\n",
    "formatted_messages = chat_prompt.format_messages(\n",
    "    role=\"物理学家\",\n",
    "    concept=\"相对论\"\n",
    ")\n",
    "print(formatted_messages)"
   ],
   "id": "fae9991068d0f714",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个专家物理学家', additional_kwargs={}, response_metadata={}), HumanMessage(content='给我解释相对论，用浅显易懂的语言', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:12:40.286689Z",
     "start_time": "2025-08-05T08:12:40.280720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "# 示例 1: 使用 BaseMessagePromptTemplate\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\"你是一个{role}.\")\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "# 示例 2: 使用 BaseMessage（已实例化的消息）\n",
    "system_msg = SystemMessage(content=\"你是一个AI工程师。\")\n",
    "human_msg = HumanMessage(content=\"你好！\")\n",
    "\n",
    "# 示例 3: 使用 BaseChatPromptTemplate（嵌套的 ChatPromptTemplate）\n",
    "nested_prompt = ChatPromptTemplate.from_messages([(\"system\", \"嵌套提示词\")])\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    system_prompt,  # MessageLike (BaseMessagePromptTemplate)\n",
    "    human_msg,      # MessageLike (BaseMessage)\n",
    "    nested_prompt,  # MessageLike (BaseChatPromptTemplate)\n",
    "])\n",
    "print(prompt.format(role=\"物理学家\", user_input=\"请给我解释黑洞\"))"
   ],
   "id": "592f90f728e0d356",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 你是一个物理学家.\n",
      "Human: 你好！\n",
      "System: 嵌套提示词\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "结合大模型的实例",
   "id": "e12f0745223b4336"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:19:36.100074Z",
     "start_time": "2025-08-05T08:19:32.492870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "######1、提供提示词#########\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个数学家，你可以计算任何算式\"),\n",
    "    (\"human\", \"{text}\"),\n",
    "])\n",
    "\n",
    "\n",
    "# 输入提示\n",
    "messages = chat_prompt.format_messages(text=\"我今年18岁，我的舅舅今年38岁，我的爷爷今年72岁，我和舅舅一共多少岁了？\")\n",
    "print(messages)\n",
    "\n",
    "######2、提供大模型#########\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"qwen-plus\",api_key=os.getenv(\"DASHSCOPE_API_KEY\"),base_url=os.getenv(\"DASHSCOPE_BASE_URL\"))\n",
    "\n",
    "######3、结合提示词，调用大模型#########\n",
    "\n",
    "# 得到模型的输出\n",
    "output = chat_model.invoke(messages)\n",
    "print(type(output))\n",
    "# 打印输出内容\n",
    "print(output.content)"
   ],
   "id": "688eb76beb354f98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='你是一个数学家，你可以计算任何算式', additional_kwargs={}, response_metadata={}), HumanMessage(content='我今年18岁，我的舅舅今年38岁，我的爷爷今年72岁，我和舅舅一共多少岁了？', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "你今年18岁，你的舅舅今年38岁。\n",
      "\n",
      "你们两个人的年龄加起来是：\n",
      "\n",
      "18岁（你） + 38岁（舅舅） = **56岁**\n",
      "\n",
      "所以，你和你的舅舅一共是 **56岁**。\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:23:26.357161Z",
     "start_time": "2025-08-05T08:23:24.773910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个数学家，你可以计算任何算式\"),\n",
    "    (\"human\", \"{text}\"),\n",
    "])\n",
    "dotenv.load_dotenv()\n",
    "chat_model = ChatOpenAI(model=\"qwen-plus\",api_key=os.getenv(\"DASHSCOPE_API_KEY\"),base_url=os.getenv(\"DASHSCOPE_BASE_URL\"))\n",
    "chain = chat_prompt | chat_model\n",
    "output = chain.invoke({\"text\": \"我今年18岁，我的舅舅今年38岁，我的爷爷今年72岁，我和舅舅一共多少岁了？\"})\n",
    "print(type(output))\n",
    "# 打印输出内容\n",
    "print(output.content)"
   ],
   "id": "d09cbb9f52191a45",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "你今年18岁，你的舅舅今年38岁。\n",
      "\n",
      "你们两个人的年龄加起来是：\n",
      "\n",
      "18岁（你） + 38岁（舅舅） = **56岁**\n",
      "\n",
      "所以，你和舅舅一共是 **56岁**。\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "插入消息列表：MessagesPlaceholder",
   "id": "6187c68344e120f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:27:18.861810Z",
     "start_time": "2025-08-05T08:27:18.857275Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "# prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n",
    "\n",
    "prompt_template.format_messages(msgs=[HumanMessage(content=\"hi!\"), HumanMessage(content=\"how are you?\"),SystemMessage(content=\"I'm good.\")])"
   ],
   "id": "9c6eafd78ad67700",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='how are you?', additional_kwargs={}, response_metadata={}),\n",
       " SystemMessage(content=\"I'm good.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 3.少量样本示例的提示模板 `FewShotPromptTemplate`或`FewShotChatMessagePromptTemplate`",
   "id": "54735e536a441f64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:53:51.522968Z",
     "start_time": "2025-08-05T08:53:50.411926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "example_prompt = PromptTemplate(\n",
    "    template=\"问题:{question}\\n{answer}\",\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    ")\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"谁活得更长，穆罕默德·阿里还是艾伦·图灵？\",\n",
    "        \"answer\": \"\"\"\n",
    "是否需要后续问题：是的。\n",
    "后续问题：穆罕默德·阿里去世时多大年纪？\n",
    "中间答案：穆罕默德·阿里去世时74岁。\n",
    "后续问题：艾伦·图灵去世时多大年纪？\n",
    "中间答案：艾伦·图灵去世时41岁。\n",
    "所以最终答案是：穆罕默德·阿里\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"克雷格斯列表的创始人是什么时候出生的？\",\n",
    "        \"answer\": \"\"\"\n",
    "是否需要后续问题：是的。\n",
    "后续问题：克雷格斯列表的创始人是谁？\n",
    "中间答案：克雷格斯列表的创始人是克雷格·纽马克。\n",
    "后续问题：克雷格·纽马克是什么时候出生的？\n",
    "中间答案：克雷格·纽马克于1952年12月6日出生。\n",
    "所以最终答案是：1952年12月6日\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"乔治·华盛顿的外祖父是谁？\",\n",
    "        \"answer\": \"\"\"\n",
    "是否需要后续问题：是的。\n",
    "后续问题：乔治·华盛顿的母亲是谁？\n",
    "中间答案：乔治·华盛顿的母亲是玛丽·波尔·华盛顿。\n",
    "后续问题：玛丽·波尔·华盛顿的父亲是谁？\n",
    "中间答案：玛丽·波尔·华盛顿的父亲是约瑟夫·波尔。\n",
    "所以最终答案是：约瑟夫·波尔\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"《大白鲨》和《皇家赌场》的导演都来自同一个国家吗？\",\n",
    "        \"answer\": \"\"\"\n",
    "是否需要后续问题：是的。\n",
    "后续问题：《大白鲨》的导演是谁？\n",
    "中间答案：《大白鲨》的导演是史蒂文·斯皮尔伯格。\n",
    "后续问题：史蒂文·斯皮尔伯格来自哪个国家？\n",
    "中间答案：美国。\n",
    "后续问题：《皇家赌场》的导演是谁？\n",
    "中间答案：《皇家赌场》的导演是马丁·坎贝尔。\n",
    "后续问题：马丁·坎贝尔来自哪个国家？\n",
    "中间答案：新西兰。\n",
    "所以最终答案是：不是\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"我的问题是：{input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# print(prompt.invoke({\"input\": \"乔治·华盛顿的父亲是谁？\"}).to_string())\n",
    "chain = prompt | chat_model\n",
    "output = chain.invoke({\"input\": \"乔治·华盛顿的父亲是谁？\"})\n",
    "print(output.content)\n",
    "print(type(output))"
   ],
   "id": "d44ac74030c7e62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否需要后续问题：是的。  \n",
      "后续问题：乔治·华盛顿的父亲是谁？  \n",
      "中间答案：乔治·华盛顿的父亲是奥古斯丁·华盛顿。  \n",
      "所以最终答案是：奥古斯丁·华盛顿\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:55:26.074313Z",
     "start_time": "2025-08-05T08:55:24.288164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "# 例子\n",
    "examples = [\n",
    "    {\"input\": \"北京天气怎么样\", \"output\": \"北京市\"},\n",
    "    {\"input\": \"南京下雨吗\", \"output\": \"南京市\"},\n",
    "    {\"input\": \"武汉热吗\", \"output\": \"武汉市\"}\n",
    "]\n",
    "\n",
    "# 例子拼装的格式\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"output\"],\n",
    "    template=\"Input: {input}\\nOutput: {output}\"\n",
    ")\n",
    "\n",
    "# Prompt模板\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Input: {input}\\nOutput:\",  # 要放在示例后面的提示模板字符串。\n",
    "    input_variables=[\"input\"]  # 传入的变量\n",
    ")\n",
    "\n",
    "prompt = prompt.format(input=\"长沙多少度\")\n",
    "\n",
    "print(\"===Prompt===\")\n",
    "print(prompt)\n",
    "\n",
    "print(\"===Response===\")\n",
    "response = chat_model.invoke(prompt)\n",
    "print(response.content)"
   ],
   "id": "6252b3ceba0be46d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Prompt===\n",
      "Input: 北京天气怎么样\n",
      "Output: 北京市\n",
      "\n",
      "Input: 南京下雨吗\n",
      "Output: 南京市\n",
      "\n",
      "Input: 武汉热吗\n",
      "Output: 武汉市\n",
      "\n",
      "Input: 长沙多少度\n",
      "Output:\n",
      "===Response===\n",
      "长沙市\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T08:57:53.801149Z",
     "start_time": "2025-08-05T08:57:53.796361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关包\n",
    "from langchain.prompts import (\n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "# 2.示例消息格式\n",
    "examples = [\n",
    "    {\"input\": \"1+1等于几？\", \"output\": \"1+1等于2\"},\n",
    "    {\"input\": \"法国的首都是？\", \"output\": \"巴黎\"}\n",
    "]\n",
    "\n",
    "# 3.定义示例的消息格式提示词模版\n",
    "msg_example_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"ai\", \"{output}\"),\n",
    "])\n",
    "\n",
    "# 4.定义FewShotChatMessagePromptTemplate对象\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=msg_example_prompt,\n",
    "    examples=examples\n",
    ")\n",
    "# 5.输出格式化后的消息\n",
    "print(few_shot_prompt.format())"
   ],
   "id": "f8ec172775c18e77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 1+1等于几？\n",
      "AI: 1+1等于2\n",
      "Human: 法国的首都是？\n",
      "AI: 巴黎\n"
     ]
    }
   ],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:00:47.218656Z",
     "start_time": "2025-08-05T09:00:46.689451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关包\n",
    "from langchain_core.prompts import (FewShotChatMessagePromptTemplate, ChatPromptTemplate)\n",
    "\n",
    "# 2.定义示例组\n",
    "examples = [\n",
    "    {\"input\": \"2🦜2\", \"output\": \"4\"},\n",
    "    {\"input\": \"2🦜3\", \"output\": \"8\"},\n",
    "]\n",
    "\n",
    "# 3.定义示例的消息格式提示词模版\n",
    "example_prompt = ChatPromptTemplate.from_messages([('human', '{input} 是多少?'), ('ai', '{output}')])\n",
    "\n",
    "# 4.定义FewShotChatMessagePromptTemplate对象\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples=examples,  # 示例组\n",
    "    example_prompt=example_prompt,  # 示例提示词词模版\n",
    ")\n",
    "# 5.输出完整提示词的消息模版\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', '你是一个数学奇才'),\n",
    "        few_shot_prompt,\n",
    "        ('human', '{input}'),\n",
    "    ]\n",
    ")\n",
    "print(final_prompt.invoke(input=\"2🦜4\"))\n",
    "\n",
    "chat_model.invoke(final_prompt.invoke(input=\"2🦜4\")).content"
   ],
   "id": "66271a36a95d7c5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='你是一个数学奇才', additional_kwargs={}, response_metadata={}), HumanMessage(content='2🦜2 是多少?', additional_kwargs={}, response_metadata={}), AIMessage(content='4', additional_kwargs={}, response_metadata={}), HumanMessage(content='2🦜3 是多少?', additional_kwargs={}, response_metadata={}), AIMessage(content='8', additional_kwargs={}, response_metadata={}), HumanMessage(content='2🦜4', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'16'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:24:24.387150Z",
     "start_time": "2025-08-05T09:24:24.383565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "\n",
    "prompt = load_prompt(\"prompt.yaml\", encoding=\"utf-8\")\n",
    "# print(prompt)\n",
    "print(prompt.format(name=\"年轻人\", what=\"滑稽\"))"
   ],
   "id": "50f968f86278b7d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请给年轻人讲一个关于滑稽的故事\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-05T09:27:55.778150Z",
     "start_time": "2025-08-05T09:27:55.775302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import load_prompt\n",
    "\n",
    "\n",
    "prompt = load_prompt(\"prompt.json\", encoding=\"utf-8\")\n",
    "# print(prompt)\n",
    "print(prompt.format(name=\"年轻人\", what=\"滑稽\"))"
   ],
   "id": "cedb49aab0fc6495",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请年轻人讲一个滑稽的故事。\n"
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
