{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 文档拆分器 Text Splitters",
   "id": "e881a522632b30a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### CharacterTextSplitter：Split by character\n",
    "\n",
    "举例1：纯字符数分割"
   ],
   "id": "89d5e674249e1731"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T08:09:01.232223Z",
     "start_time": "2025-08-08T08:09:01.228611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关依赖\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 2.示例文本\n",
    "text = \"\"\"\n",
    "LangChain 是一个用于开发由语言模型驱动的应用程序的框架的。它提供了一套工具和抽象，使开发者能够更容易地构建复杂的应用程序。\n",
    "\"\"\"\n",
    "\n",
    "# 3.定义字符分割器\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=50, # 每块大小\n",
    "    chunk_overlap=5,# 块与块之间的重复字符数\n",
    "    # length_function=len, # 用于计算切块长度的方法。默认只计算字符数，但通常这里会使用Token。\n",
    "    separator=\"\"   # 禁用分隔符优先\n",
    ")\n",
    "\n",
    "# 4.分割文本\n",
    "texts = splitter.split_text(text)\n",
    "\n",
    "# 5.打印结果\n",
    "for i, chunk in enumerate(texts):\n",
    "    print(f\"块 {i+1}:长度：{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"-\" * 50)"
   ],
   "id": "ceac2fda0f98c434",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 1:长度：49\n",
      "LangChain 是一个用于开发由语言模型驱动的应用程序的框架的。它提供了一套工具和抽象，使开发\n",
      "--------------------------------------------------\n",
      "块 2:长度：22\n",
      "象，使开发者能够更容易地构建复杂的应用程序。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2：指定分割符",
   "id": "70f19a46faf86efd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T08:14:24.482529Z",
     "start_time": "2025-08-08T08:14:24.478309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关依赖\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 2.定义要分割的文本\n",
    "text = \"这是一个示例文本啊。我们将使用CharacterTextSplitter将其分割成小块。分割基于字符数。123。\"\n",
    "\n",
    "# text = \"\"\"\n",
    "# LangChain 是一个用于开发由语言模型。驱动的应用程序的框架的。它提供了一套工具和抽象。使开发者能够更容易地构建复杂的应用程序。\n",
    "# \"\"\"\n",
    "\n",
    "# 3.定义分割器实例\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=43,   # 每个块的最大字符数\n",
    "    chunk_overlap=5, # 块之间的重叠字符数\n",
    "    separator=\"。\",  # 按句号分割\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "# 4.开始分割\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# 5.打印效果\n",
    "for  i,chunk in enumerate(chunks):\n",
    "    print(f\"块 {i + 1}:长度：{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"-\"*50)\n"
   ],
   "id": "2dab400f8fda810",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 1:长度：43\n",
      "这是一个示例文本啊。我们将使用CharacterTextSplitter将其分割成小块\n",
      "--------------------------------------------------\n",
      "块 2:长度：13\n",
      "。分割基于字符数。123。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**注意：无重叠。**\n",
    "\n",
    "**separator优先原则**：当设置了 `separator`（如\"。\"），分割器会首先尝试在分隔符处分割，然后再考虑 chunk_size。这是为了避免在句子中间硬性切断。这种设计是为了：\n",
    "\n",
    "1. 优先保持语义完整性（不切断句子）\n",
    "2. 避免产生无意义的碎片（如半个单词/不完整句子）\n",
    "3. chunk_overlap仅在合并后的片段之间生效（如果 `chunk_size` 足够大）。如果没有合并的片段，则 overlap失效。\n",
    "4. 如果 `chunk_size` 比片段小，无法拆分片段，导致 overlap失效。"
   ],
   "id": "f71fe26101586853"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T08:17:30.431230Z",
     "start_time": "2025-08-08T08:17:30.427780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关依赖\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 2.定义要分割的文本\n",
    "text = \"这是第一段文本。这是第二段内容。最后一段结束。\"\n",
    "\n",
    "# 3.定义字符分割器\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"。\",\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=10,\n",
    "    keep_separator=True\n",
    ")\n",
    "\n",
    "# 4.分割文本\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# 5.打印结果\n",
    "for  i,chunk in enumerate(chunks):\n",
    "    print(f\"块 {i + 1}:长度：{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"-\"*50)"
   ],
   "id": "4ee74e3cf4638a21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "块 1:长度：15\n",
      "这是第一段文本。这是第二段内容\n",
      "--------------------------------------------------\n",
      "块 2:长度：16\n",
      "。这是第二段内容。最后一段结束。\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### RecursiveCharacterTextSplitter：最常用\n",
    "\n",
    "文档切分器中较常用的是`RecursiveCharacterTextSplitter`，遇到`特定字符`时进行分割。默认情况下，它尝试进行切割的字符包括 `[\"\\n\\n\", \"\\n\", \" \", \"\"]`。此外，还可以考虑添加，。等分割字符。\n",
    "\n",
    "具体为：该文本分割器接受一个字符列表作为参数，根据第一个字符进行切块，但如果任何切块太大，则会继续移动到下一个字符，并以此类推。\n",
    "\n",
    "\n",
    "\n",
    "**特点**：\n",
    "\n",
    "- **智能分段**：通过递归尝试多种分隔符（如换行符、句号、空格等），将文本分割为大小接近chunk_size的片段，同时避免在句子中间截断。\n",
    "- **保留上下文**：优先在自然语言边界（如段落、句子结尾）处分割，减少信息碎片化。\n",
    "- **灵活适配**：适用于多种文本类型（代码、Markdown、普通文本等），是LangChain中最通用的文本分割器。\n",
    "\n",
    "除此之外，还可以指定的参数包括：\n",
    "\n",
    "- `length_function`：用于计算切块长度的方法。默认只计算字符数，但通常这里会使用Token。\n",
    "- `chunk_size`：每个切块的最大token数量（由长度函数测量）。\n",
    "- `chunk_overlap`：相邻两个切块之间的最大重叠token数量。设置重叠可以使信息不会因为在边界处被切断而丢失。\n",
    "- `add_start_index`：是否在元数据中包含每个切块在原始文档中的起始位置。"
   ],
   "id": "6fb5d146eba8d5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T08:35:20.505872Z",
     "start_time": "2025-08-08T08:35:20.501398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关依赖\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2.定义RecursiveCharacterTextSplitter分割器对象\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=60,  # 每个块中的最大字符数（非单词或Token）\n",
    "    chunk_overlap=0, # 邻块之间重叠的字符数。重叠的块可以确保如果重要信息横跨两个块，它不会被错过。一般设为chunk_size的10-20%\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# 3.定义分割的内容\n",
    "list=[\"LangChain框架特性\\n\\n多模型集成(GPT/Claude)\\n记忆管理功能\\n链式调用设计。文档分析场景示例：需要处理PDF/Word等格式。\"]\n",
    "\n",
    "# 4.分割器分割\n",
    "paragraphs = text_splitter.create_documents(list)\n",
    "\n",
    "for para in paragraphs:\n",
    "    print(para)\n",
    "    print('-------')"
   ],
   "id": "b9831034257345ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='LangChain框架特性' metadata={'start_index': 0}\n",
      "-------\n",
      "page_content='多模型集成(GPT/Claude)\n",
      "记忆管理功能\n",
      "链式调用设计。文档分析场景示例：需要处理PDF/Word等格式。' metadata={'start_index': 15}\n",
      "-------\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T08:36:54.379942Z",
     "start_time": "2025-08-08T08:36:54.375717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关依赖\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2.打开.txt文件\n",
    "with open(\"asset/load/08-ai.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "\n",
    "# 3.定义RecursiveCharacterTextSplitter（递归字符分割器）\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# 4.分割文本\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "# 5.打印分割文本\n",
    "for text in texts:\n",
    "    print(text)\n"
   ],
   "id": "4050c3539f95eb21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='人工智能（AI）是什么？'\n",
      "page_content='人工智能（Artificial'\n",
      "page_content='Intelligence，简称AI）是指由计算机系统模拟人类智能的技术，使其能够执行通常需要人类认知能力的任务，如学习、推理、决策和语言理解。AI的核心目标是让机器具备感知环境、处理信息并自主行动的'\n",
      "page_content='让机器具备感知环境、处理信息并自主行动的能力。'\n",
      "page_content='1. AI的技术基础\n",
      "AI依赖多种关键技术：\n",
      "\n",
      "机器学习（ML）：通过算法让计算机从数据中学习规律，无需显式编程。例如，推荐系统通过用户历史行为预测偏好。'\n",
      "page_content='深度学习：基于神经网络的机器学习分支，擅长处理图像、语音等复杂数据。AlphaGo击败围棋冠军便是典型案例。\n",
      "\n",
      "自然语言处理（NLP）：使计算机理解、生成人类语言，如ChatGPT的对话能力。'\n",
      "page_content='2. AI的应用场景\n",
      "AI已渗透到日常生活和各行各业：\n",
      "\n",
      "医疗：辅助诊断（如AI分析医学影像）、药物研发加速。\n",
      "\n",
      "交通：自动驾驶汽车通过传感器和AI算法实现安全导航。'\n",
      "page_content='金融：欺诈检测、智能投顾（如风险评估模型）。\n",
      "\n",
      "教育：个性化学习平台根据学生表现调整教学内容。\n",
      "\n",
      "3. AI的挑战与未来\n",
      "尽管前景广阔，AI仍面临问题：'\n",
      "page_content='伦理争议：数据隐私、算法偏见（如招聘AI歧视特定群体）。\n",
      "\n",
      "就业影响：自动化可能取代部分人工岗位，但也会创造新职业。\n",
      "\n",
      "技术瓶颈：通用人工智能（AGI）尚未实现，当前AI仅擅长特定任务。'\n",
      "page_content='未来，AI将与人类协作而非替代：医生借助AI提高诊断效率，教师利用AI定制课程。其发展需平衡技术创新与社会责任，确保技术造福全人类。'\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "利用PDFLoader加载文档，对文档的内容用递归切割器切割",
   "id": "8887a958a3160cf4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T09:53:41.829764Z",
     "start_time": "2025-08-08T09:53:41.409783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.导入相关依赖\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2.定义PyPDFLoader加载器\n",
    "loader = PyPDFLoader(\"/Users/zhangyf/Documents/简历/大模型v3.0/张一帆_大模型算法工程师.pdf\")\n",
    "\n",
    "# 3.加载和切割文档对象\n",
    "docs = loader.load()\n",
    "# print(f\"第0页：\\n{pages[0].page_content}\")\n",
    "\n",
    "# 4.定义切割器\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# 5.对pdf内容进行切割得到文档对象\n",
    "paragraphs = text_splitter.create_documents([docs[1].page_content])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('-------')"
   ],
   "id": "eb2540a07340c867",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 33 0 (offset 0)\n",
      "Ignoring wrong pointing object 39 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(LoRA)  随着水泥工业智能化发展，大量生产场景需要更专业的自然语言处理支持，例如工艺文档分析、生产数据解释、设备异常预警等。为此基于Qwen开源模型，进行了针对工业领域的模型微调，增强其在垂直领域中的语义理解与问答能力，为水泥工厂提供更精准的知识服务和决策支持\n",
      "-------\n",
      "l 收集工业相关技术文档及案例数据，包括设备手册、工艺流程说明、历史问题记录 l 使用Alpaca数据集并结合Langchain工具对文档进行清洗和格式化处理，将文档转换为模型训练所需的指令数据集 l 基于LLaMA-Factory进行工业场景的QLoRA微调训练，包括设备术语、工艺流程等领域的适配优化 l 执行工业语料的增量微调训练，提升模型对特定工业领域术语和文档的语义理解能力 l\n",
      "-------\n",
      "l 基于LLaMA-Factory进行工业场景的QLoRA微调训练，包括设备术语、工艺流程等领域的适配优化 l 执行工业语料的增量微调训练，提升模型对特定工业领域术语和文档的语义理解能力 l 执行微调，并将微调后的模型合并保存，方便本地使用  l 使用 vLLM加载微调好的模型，测试生成效果\n",
      "-------\n",
      "l 针对水泥工厂的工艺流程和设备术语，模型生成结果上下文关联性显著提高，满足复杂场景需求 l 采用QLoRA微调技术，显著降低训练和推理资源占用，适配本地硬件环境 l 微调后的模型对垂直领域支持度更好，泛化性提高，回答精准，不易出现幻觉  (RAG)\n",
      "-------\n",
      "随着水泥工业智能工厂设备类型和工艺流程的日益复杂，生产和运维过程中积累了大量文档数据，包括操作手册、设备维护记录、技术标准和历史故障数据。这些数据具有动态增长的特性，为了更好地挖掘其价值，开发了一套基于大语言模型（LLM）的知识问答系统。系统结合检索增强生成（RAG）技术，支持对动态更新的工业知识进行高效检索与智能问答，为操作员和工程师提供实时支持。针对水泥工业生产中常见的设备异常、工艺偏差和生\n",
      "-------\n",
      "基于大语言模型（LLM）的知识问答系统。系统结合检索增强生成（RAG）技术，支持对动态更新的工业知识进行高效检索与智能问答，为操作员和工程师提供实时支持。针对水泥工业生产中常见的设备异常、工艺偏差和生产故障等固定问题场景，系统通过微调训练本地部署的\n",
      "-------\n",
      "LLM 模型，将领域知识嵌入模型，进一步优化模型在固定问题上的推理能力。通过这种动态检索和固定知识微调\n",
      "-------\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "####  Split by tokens(了解)\n",
    "\n",
    "大语言模型通常都有一个最大的输入Token限制。因此，当我们将文本拆分为块时，除了字符以外，更常用的一种方法计算Token的数量。此外，大语言模型(LLM)通常是以token的数量作为其计量(或收费)的依据，所以采用token分割也有助于我们在使用时更好的控制成本。\n",
    "\n",
    "**\"Split by tokens\"**（按Token分割）是指根据文本的`Token数量`（而非字符或单词数）将长文本切分成多个小块。这是为了适配大多数语言模型（如GPT）的输入限制（如4096 Token），确保每个文本块不超过模型的Token上限。\n",
    "\n",
    "这里提供一个Token与字符转化的可视化工具，有OpenAI提供：https://platform.openai.com/tokenizer\n",
    "\n",
    "1. **什么是Token？**\n",
    "   - 对模型而言，Token是文本的最小处理单位。例如：\n",
    "     - 英文：`\"hello\"` → 1个Token，`\"ChatGPT\"` → 2个Token（`\"Chat\"` + `\"GPT\"`）。\n",
    "     - 中文：`\"人工智能\"` → 可能拆分为2-3个Token（取决于分词器）。\n",
    "2. **为什么按Token分割？**\n",
    "   - 语言模型对输入长度的限制是基于Token数（如GPT-4的8k/32k Token上限）。\n",
    "   - 直接按字符或单词分割可能导致实际Token数超限。\n",
    "\n"
   ],
   "id": "bb9f7c10ddd3dcb4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T02:41:19.653114Z",
     "start_time": "2025-08-09T02:25:40.795603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 导入相关依赖\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import tiktoken  # 用于计算Token数量\n",
    "\n",
    "\n",
    "# 2. 定义通过Token切割器\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=32,\n",
    "    chunk_overlap=0,\n",
    "    separator=\"。\",  # 指定中文句号为分隔符\n",
    "    keep_separator=False,  # 保留句号\n",
    ")\n",
    "# 3.定义文本\n",
    "text = \"人工智能是一个强大的开发框架。它支持多种语言模型和工具链。今天你吃了吗？我好想吃饭，但是现在不饿 咋办 12345不说话\"\n",
    "\n",
    "# 4. 开始切割\n",
    "texts = text_splitter.split_text(text)\n",
    "print(f\"分割后的块数: {len(texts)}\")\n",
    "\n",
    "# 5. 初始化tiktoken编码器（用于Token计数）\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\")  # 确保与CharacterTextSplitter的encoding_name一致\n",
    "\n",
    "# 6. 打印每个块的Token数和内容\n",
    "for i, chunk in enumerate(texts):\n",
    "    tokens = encoder.encode(chunk)  # 现在encoder已定义\n",
    "    print(f\"块 {i + 1}: {len(tokens)} Token\\n内容: {chunk}\\n\")"
   ],
   "id": "e97331535c06ceb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分割后的块数: 2\n",
      "块 1: 32 Token\n",
      "内容: 人工智能是一个强大的开发框架。它支持多种语言模型和工具链\n",
      "\n",
      "块 2: 37 Token\n",
      "内容: 今天你吃了吗？我好想吃饭，但是现在不饿 咋办 12345不说话\n",
      "\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T01:12:51.627577Z",
     "start_time": "2025-08-11T01:12:47.760016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 加载文本\n",
    "with open(\"asset/load/09-ai1.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "embed_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\")\n",
    ")\n",
    "\n",
    "# 获取切割器\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embed_model,\n",
    "    breakpoint_threshold_type=\"percentile\",#断点阈值类型：字面值[\"百分位数\", \"标准差\", \"四分位距\", \"梯度\"] 选其一\n",
    "    breakpoint_threshold_amount=0.1 #断点阈值数量\n",
    ")\n",
    "\n",
    "# 切分文档\n",
    "docs = text_splitter.create_documents(texts = [state_of_the_union])\n",
    "print(len(docs))\n",
    "for doc in docs:\n",
    "    print(f\"🔍 文档 {doc}:\")"
   ],
   "id": "54c5c4ebd6c65535",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/langchain/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "🔍 文档 page_content='人工智能综述：发展、应用与未来展望\n",
      "\n",
      "摘要\n",
      "人工智能（Artificial Intelligence，AI）作为计算机科学的一个重要分支，近年来取得了突飞猛进的发展。本文综述了人工智能的发展历程、核心技术、应用领域以及未来发展趋势。通过对人工智能的定义、历史背景、主要技术（如机器学习、深度学习、自然语言处理等）的详细介绍，探讨了人工智能在医疗、金融、教育、交通等领域的应用，并分析了人工智能发展过程中面临的挑战与机遇。最后，本文对人工智能的未来发展进行了展望，提出了可能的突破方向。\n",
      "\n",
      "1. 引言\n",
      "人工智能是指通过计算机程序模拟人类智能的一门科学。自20世纪50年代诞生以来，人工智能经历了多次起伏，近年来随着计算能力的提升和大数据的普及，人工智能技术取得了显著的进展。人工智能的应用已经渗透到日常生活的方方面面，从智能手机的语音助手到自动驾驶汽车，从医疗诊断到金融分析，人工智能正在改变着人类社会的运行方式。\n",
      "\n",
      "2.':\n",
      "🔍 文档 page_content='人工智能的发展历程\n",
      "2.1 早期发展\n",
      "人工智能的概念最早可以追溯到20世纪50年代。1956年，达特茅斯会议（Dartmouth Conference）被认为是人工智能研究的正式开端。在随后的几十年里，人工智能研究经历了多次高潮与低谷。早期的研究主要集中在符号逻辑和专家系统上，但由于计算能力的限制和算法的不足，进展缓慢。\n",
      "2.2 机器学习的兴起\n",
      "20世纪90年代，随着统计学习方法的引入，机器学习逐渐成为人工智能研究的主流。支持向量机（SVM）、决策树、随机森林等算法在分类和回归任务中取得了良好的效果。这一时期，机器学习开始应用于数据挖掘、模式识别等领域。\n",
      "2.3 深度学习的突破\n",
      "2012年，深度学习在图像识别领域取得了突破性进展，标志着人工智能进入了一个新的阶段。深度学习通过多层神经网络模拟人脑的工作方式，能够自动提取特征并进行复杂的模式识别。卷积神经网络（CNN）、循环神经网络（RNN）和长短期记忆网络（LSTM）等深度学习模型在图像处理、自然语言处理、语音识别等领域取得了显著成果。\n",
      "\n",
      "3.':\n",
      "🔍 文档 page_content='人工智能的核心技术\n",
      "3.1 机器学习\n",
      "机器学习是人工智能的核心技术之一，通过算法使计算机从数据中学习并做出决策。常见的机器学习算法包括监督学习、无监督学习和强化学习。监督学习通过标记数据进行训练，无监督学习则从未标记数据中寻找模式，强化学习则通过与环境交互来优化决策。\n",
      "3.2 深度学习\n",
      "深度学习是机器学习的一个子领域，通过多层神经网络进行特征提取和模式识别。深度学习在图像识别、自然语言处理、语音识别等领域取得了显著成果。常见的深度学习模型包括卷积神经网络（CNN）、循环神经网络（RNN）和长短期记忆网络（LSTM）。\n",
      "3.3 自然语言处理\n",
      "自然语言处理（NLP）是人工智能的一个重要分支，致力于使计算机能够理解和生成人类语言。NLP技术广泛应用于机器翻译、情感分析、文本分类等领域。近年来，基于深度学习的NLP模型（如BERT、GPT）在语言理解任务中取得了突破性进展。\n",
      "3.4 计算机视觉\n",
      "计算机视觉是人工智能的另一个重要分支，致力于使计算机能够理解和处理图像和视频。计算机视觉技术广泛应用于图像识别、目标检测、人脸识别等领域。深度学习模型（如CNN）在计算机视觉任务中取得了显著成果。\n",
      "\n",
      "4.':\n",
      "🔍 文档 page_content='人工智能的应用领域\n",
      "4.1 医疗健康\n",
      "人工智能在医疗健康领域的应用包括疾病诊断、药物研发、个性化医疗等。通过分析医学影像和患者数据，人工智能可以帮助医生更准确地诊断疾病，提高治疗效果。\n",
      "4.2 金融\n",
      "人工智能在金融领域的应用包括风险评估、欺诈检测、算法交易等。通过分析市场数据和交易记录，人工智能可以帮助金融机构做出更明智的决策，提高运营效率。\n",
      "4.3 教育\n",
      "人工智能在教育领域的应用包括个性化学习、智能辅导、自动评分等。通过分析学生的学习数据，人工智能可以为学生提供个性化的学习建议，提高学习效果。\n",
      "4.4 交通\n",
      "人工智能在交通领域的应用包括自动驾驶、交通管理、智能导航等。通过分析交通数据和路况信息，人工智能可以帮助优化交通流量，提高交通安全。\n",
      "\n",
      "5.':\n",
      "🔍 文档 page_content='人工智能的挑战与机遇\n",
      "5.1 挑战\n",
      "人工智能发展过程中面临的主要挑战包括数据隐私、算法偏见、安全性问题等。数据隐私问题涉及到个人数据的收集和使用，算法偏见问题则涉及到算法的公平性和透明度，安全性问题则涉及到人工智能系统的可靠性和稳定性。\n",
      "5.2 机遇\n",
      "尽管面临挑战，人工智能的发展也带来了巨大的机遇。人工智能技术的进步将推动各行各业的创新，提高生产效率，改善生活质量。未来，人工智能有望在更多领域取得突破，为人类社会带来更多的便利和福祉。\n",
      "\n",
      "6.':\n",
      "🔍 文档 page_content='未来展望\n",
      "6.1 技术突破\n",
      "未来，人工智能技术有望在以下几个方面取得突破：一是算法的优化和创新，提高模型的效率和准确性；二是计算能力的提升，支持更复杂的模型和更大规模的数据处理；三是跨学科研究的深入，推动人工智能与其他领域的融合。\n",
      "6.2 应用拓展\n",
      "随着技术的进步，人工智能的应用领域将进一步拓展。未来，人工智能有望在更多领域发挥重要作用，如环境保护、能源管理、智能制造等。人工智能将成为推动社会进步的重要力量。\n",
      "\n",
      "7.':\n",
      "🔍 文档 page_content='结论\n",
      "人工智能作为一门快速发展的科学，正在改变着人类社会的运行方式。通过不断的技术创新和应用拓展，人工智能将为人类社会带来更多的便利和福祉。然而，人工智能的发展也面临着诸多挑战，需要社会各界共同努力，推动人工智能的健康发展。':\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T01:22:25.347376Z",
     "start_time": "2025-08-11T01:22:24.102659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "# 初始化嵌入模型\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\",api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
    "\n",
    "# 待嵌入的文本句子\n",
    "text = \"What was the name mentioned in the conversation?\"\n",
    "\n",
    "# 生成一个嵌入向量\n",
    "embedded_query = embeddings_model.embed_query(text = text)\n",
    "\n",
    "# 使用embedded_query[:5]来查看前5个元素的值\n",
    "print(len(embedded_query))\n",
    "print(embedded_query[:5])"
   ],
   "id": "17b4ef7314184e05",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n",
      "[0.0058608162216842175, 0.04196786880493164, -0.02676955610513687, 0.007899938151240349, -0.030308160930871964]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T01:31:06.672176Z",
     "start_time": "2025-08-11T01:31:04.303509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "\n",
    "# 初始化嵌入模型\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-large\",api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENAI_BASE_URL\"))\n",
    "\n",
    "# 待嵌入的文本列表\n",
    "texts = [\n",
    "    \"Hi there!\",\n",
    "    \"Oh, hello!\",\n",
    "    \"What's your name?\",\n",
    "    \"My friends call me World\",\n",
    "    \"Hello World!\"\n",
    "]\n",
    "\n",
    "# 生成嵌入向量\n",
    "embeddings = embeddings_model.embed_documents(texts)\n",
    "\n",
    "\n",
    "for i in range(len(texts)):\n",
    "    print(f\"{texts[i]}:{embeddings[i][:3]}\",end=\"\\n\\n\")"
   ],
   "id": "6ba7fdeca553c150",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi there!:[-0.039683159440755844, -0.0036716836038976908, -0.026338515803217888]\n",
      "\n",
      "Oh, hello!:[-0.019019540399312973, -0.0005694996798411012, -0.032529160380363464]\n",
      "\n",
      "What's your name?:[-0.02662552148103714, -0.0008320475462824106, -0.025211438536643982]\n",
      "\n",
      "My friends call me World:[0.01848779059946537, 0.0008859765366651118, -0.005505858920514584]\n",
      "\n",
      "Hello World!:[0.0051402803510427475, -0.016360284760594368, -0.016105936840176582]\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
